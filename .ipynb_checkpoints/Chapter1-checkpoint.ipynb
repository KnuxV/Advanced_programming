{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e5b3555",
   "metadata": {},
   "source": [
    "# Chapter 1: Webscrapping and api.\n",
    "\n",
    "In this chapter we will go more in depth on the scraping methodology. First we will go back to Beautiful Soup with a more complex example, we then discuss the advantages of selenium and scrappy and move on to the API part of the course. \n",
    "\n",
    "Structure:\n",
    "- [Web developping tools in the webbrowser](#WB)\n",
    "- [Beautiful Soup](#BS)\n",
    "- [Scrapy](#Scrapy)\n",
    "- [Selenium](#Selenium)\n",
    "- [APIs](#APIs)\n",
    "- [Rules of good conduct](#rules)\n",
    "- [TODO](#TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae71af22",
   "metadata": {},
   "source": [
    "<a name=\"WB\"></a>\n",
    "## Web developping tools in the webbrowser\n",
    "\n",
    "We have seen in introduction how to download html and with Beautiful Soup (BS) and read it with prettify. In practice it can be hard to find what you want using this method. An other option is to use web developping tools available on every browser (Google chrome, Mozilla, ...). Here's a quick introduction to what you can do on browser and how it can help you (note: i'll be using Mozilla)\n",
    "\n",
    "- Ctrl+u   -> Watching the source code generating the page (What we get with requests.get() )\n",
    "- Ctrl+Maj+C -> Inspector, Hover on element to see where it is on html page\n",
    "- Maj+F7 -> Style editor, Check the css the page is using\n",
    "- Ctrl+Maj+E -> Network, see what you are loading when opening a page (important for JS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30979d8e",
   "metadata": {},
   "source": [
    "<a name=\"BS\"></a>\n",
    "## Beautiful Soup\n",
    "\n",
    "So we have seen the basic usage of BS as a reminder of last year, let's move on a bigger project. You are probably familiar with the 6 degrees of separation ? Number of \"steps\" (friends of friends) between two individuals is 6 or fewer. Our goal will be to scrap a wikipedia page, get all the href and continue this process until 6 layers deep. (This idea comes from Mitchell R. Web scraping with Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "830a4ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import re # regex expression\n",
    "import tqdm.notebook as tq # time loop in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c28068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from the wikipedia page of Kevin Bacon\n",
    "starting_url = \"https://en.wikipedia.org/wiki/Kevin_Bacon\"\n",
    "\n",
    "# Get html content\n",
    "response = requests.get(starting_url)\n",
    "result = response.content\n",
    "\n",
    "# Parse html with BS\n",
    "soup = BeautifulSoup(result, 'html.parser')\n",
    "\n",
    "# In the body content find all href that matches the regex query (start with wiki and ignore !: to avoid artifacts like jpeg )\n",
    "for link in soup.find(\"div\",attrs={'id':'bodyContent'}).find_all(\"a\",href = re.compile(\"^(/wiki/)((?!:).)*$\")):\n",
    "    print(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c24a1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using function so that it is cleaner\n",
    "\n",
    "\n",
    "def Get_hrefs(url):\n",
    "    # Request url and create bs object.\n",
    "    response = requests.get(url)\n",
    "    result = response.content    \n",
    "    soup = BeautifulSoup(result, 'html.parser')\n",
    "    \n",
    "    # init the list with all href\n",
    "    hrefs = []\n",
    "    for link in soup.find(\"div\",attrs={'id':'bodyContent'}).find_all(\"a\",href = re.compile(\"^(/wiki/)((?!:).)*$\")):\n",
    "        if \"href\" in link.attrs:\n",
    "            if link.get(\"href\") not in hrefs:\n",
    "                hrefs.append(link.get(\"href\"))\n",
    "    return(hrefs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c62d930d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42da53f8e8e4e719e93c276721e0a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe23c97b949d4bba9038136392834861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/273 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# depth = number of times we get the hrefs of the hrefs.\n",
    "# We limit at 2 to not overlead wikipedia with our things but in theory depth of 6 and you could have every person ?\n",
    "depth = 2\n",
    "\n",
    "# hrefs_checked = keeping track of href already visited\n",
    "hrefs_checked = []\n",
    "\n",
    "for i in tq.tqdm(range(depth)):\n",
    "    # First iteration start from Kevin Bacon\n",
    "    if i == 0:\n",
    "        starting_url = \"https://en.wikipedia.org/wiki/Kevin_Bacon\"\n",
    "        hrefs = Get_hrefs(starting_url)\n",
    "        hrefs_checked.append(starting_url)\n",
    "    else:\n",
    "        hrefs_temp = []\n",
    "        for starting_url in tq.tqdm(hrefs):\n",
    "            url = \"https://en.wikipedia.org\" + starting_url\n",
    "            # Checking if url not visited. Could become inneficient\n",
    "            if url not in hrefs_checked:\n",
    "                hrefs_temp += Get_hrefs(url)\n",
    "        hrefs = [href for href in hrefs_temp if href not in hrefs_checked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5f8f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A step further we want to process text and save it in MongoDB\n",
    "# Also short intro into classes\n",
    "import pymongo \n",
    "\n",
    "class crawler:\n",
    "    def __init__(self,starting_url, depth, mongo_uri, db_name, collection_name ):\n",
    "        self.starting_url = starting_url\n",
    "        self.depth = depth\n",
    "        self.mongo_uri = mongo_uri\n",
    "        self.db_name = db_name\n",
    "        self.collection_name = collection_name\n",
    "        self.hrefs_checked = []\n",
    "        self.n_processed = 0\n",
    "        \n",
    "    def Get_hrefs(self,url):\n",
    "\n",
    "        hrefs = []\n",
    "        for link in self.soup.find(\"div\",attrs={'id':'bodyContent'}).find_all(\"a\",href = re.compile(\"^(/wiki/)((?!:).)*$\")):\n",
    "            if \"href\" in link.attrs:\n",
    "                if link.get(\"href\") not in hrefs:\n",
    "                    hrefs.append(link.get(\"href\"))\n",
    "        return(hrefs)\n",
    "    \n",
    "    def parse_url(self): \n",
    "        full_text = \"\"\n",
    "        for para in self.soup.find_all(\"p\"):\n",
    "            full_text += para.text + \" \"\n",
    "        return(full_text)\n",
    "    \n",
    "    def save2mongo(self):\n",
    "        Client = pymongo.MongoClient(self.mongo_uri)\n",
    "        db = Client[self.db_name]\n",
    "        collection = db[self.collection_name]\n",
    "        \n",
    "        collection.insert_many(self.list_of_insertion)\n",
    "        \n",
    "    def run_analysis(self):\n",
    "        \n",
    "        self.list_of_insertion = []\n",
    "        \n",
    "        for i in tq.tqdm(range(self.depth)):\n",
    "            # First iteration start from Kevin Bacon\n",
    "            if i == 0:\n",
    "                response = requests.get(self.starting_url)\n",
    "                result = response.content    \n",
    "                self.soup = BeautifulSoup(result, 'html.parser')\n",
    "                hrefs = self.Get_hrefs(self.starting_url)\n",
    "                text = self.parse_url()\n",
    "                self.hrefs_checked.append(self.starting_url)\n",
    "                self.n_processed += 1\n",
    "                self.list_of_insertion.append({\"id\":self.n_processed, \"text\" : text, \"href\":self.starting_url})\n",
    "            else:\n",
    "                hrefs_temp = []\n",
    "                for starting_url in tq.tqdm(hrefs):\n",
    "                    url = \"https://en.wikipedia.org\" + starting_url\n",
    "                    response = requests.get(url)\n",
    "                    result = response.content    \n",
    "                    self.soup = BeautifulSoup(result, 'html.parser')\n",
    "                    hrefs_temp += self.Get_hrefs(url)\n",
    "                    text = self.parse_url()\n",
    "                    self.n_processed += 1\n",
    "                    self.list_of_insertion.append({\"id\":self.n_processed, \"text\" : text, \"href\":url})\n",
    "                    if len(self.list_of_insertion) % 200 == 0:\n",
    "                        self.save2mongo()\n",
    "                        self.list_of_insertion = []\n",
    "                hrefs = [href for href in hrefs_temp if href not in hrefs_checked]      \n",
    "        self.save2mongo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7242fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl = crawler(starting_url=\"https://en.wikipedia.org/wiki/Kevin_Bacon\", depth = 3, mongo_uri = 'mongodb://localhost:27017', db_name = \"M2\", collection_name=\"BS\")\n",
    "crawl.run_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25949aa7",
   "metadata": {},
   "source": [
    "<a name=\"Scrapy\"></a>\n",
    "## Scrapy\n",
    "\n",
    "Although BS works well on small examples it requires an extra amount of work on larger project to have it well structured. This overhead can be avoided using Scrapy which is another Python webscraping library. Also you can't use Xpaths in BS which are a cleaner way to find elements. The entry cost to scrapy is high but once mastered it will help you a lot in your scraping work. We will try to reproduce the BS wikipedia code but using scrapy. As always installation is straightforward:\n",
    "\n",
    "```console\n",
    "pip install scrapy\n",
    "```\n",
    "\n",
    "Scrapy works by first creating a project. Go to a folder that will have the project inside and run the following in a terminal/cmd prompt:\n",
    "\n",
    "```console\n",
    "scrapy startproject scrapyap\n",
    "```\n",
    "\n",
    "For the moment don't look too much into the folder created, we first want to create a script called \"spider\" in scrapy terminology which will be your main script at the beginning:\n",
    "\n",
    "```console\n",
    "cd scrapyap\n",
    "scrapy genspider spider_wikipedia wikipedia.org\n",
    "```\n",
    "\n",
    "At the end you should have the following structure\n",
    "\n",
    "```\n",
    "scrapy.cfg\n",
    "scrapy_ap\n",
    "│   \n",
    "└───spiders\n",
    "│   │   __init__.py\n",
    "│   │   spider_wikipedia.py\n",
    "│   __init__.py    \n",
    "│   items.py\n",
    "│   middlewares.py\n",
    "│   pipelines.py\n",
    "│   settings.py\n",
    "```\n",
    "\n",
    "At any point in the process of writing code you can use something called scrapy shell. This allows you to do some small examples and test without having to run the whole thing.\n",
    "\n",
    "```console\n",
    "scrapy shell\n",
    "fetch(\"https://en.wikipedia.org/wiki/Kevin_Bacon\")\n",
    "view(response)\n",
    "hrefs = response.xpath(\"//div[@id='bodyContent']//a[@href[re:test(.,'^(/wiki/)((?!:).)*$')]]/@href\").getall()\n",
    "print(hrefs)\n",
    "```\n",
    "\n",
    "There's a lot to go through so to start let's focus on spider_wikipedia.py, it should look like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1562304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider_wikipedia.py \n",
    "\n",
    "import scrapy\n",
    "\n",
    "# A class that inherits from scrapy.Spider. We will see in CHap 2 what inheritance is for the moment just know that we \"inherit\" modules from the class scrapy.Spider\n",
    "# This means that you have some function and features already implemented and usable. \n",
    "class SpiderWikipediaSpider(scrapy.Spider):\n",
    "    # the name we introduce during the creation of the spider\n",
    "    name = 'spider_wikipedia'\n",
    "    # If you try to scrap an url outside of allowed_domains it wont work\n",
    "    allowed_domains = ['wikipedia.org']\n",
    "    # The first url you will parse\n",
    "    start_urls = ['http://wikipedia.org/']\n",
    "\n",
    "    # What you do with the first url, response = what we get with a request.get()\n",
    "    def parse(self, response):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec0dca9",
   "metadata": {},
   "source": [
    "Let's start small, how do we change this code to get the hrefs and urls and iterate this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effbac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider_wikipedia.py \n",
    "\n",
    "import scrapy\n",
    "import time\n",
    "# A class that inherits from scrapy.Spider. We will see in CHap 2 what inheritance is for the moment just know that we \"inherit\" modules from the class scrapy.Spider\n",
    "# This means that you have some function and features already implemented and usable. \n",
    "class SpiderWikipediaSpider(scrapy.Spider):\n",
    "    # the name we introduce during the creation of the spider\n",
    "    name = 'spider_wikipedia'\n",
    "    # If you try to scrap an url outside of allowed_domains it wont work\n",
    "    allowed_domains = ['wikipedia.org']\n",
    "    # The first url you will parse\n",
    "    start_urls = [\"https://en.wikipedia.org/wiki/Kevin_Bacon\"]\n",
    "\n",
    "    # What you do with the first url, response = what we get with a request.get()\n",
    "    def parse(self, response):\n",
    "        # time.sleep because we are nice.\n",
    "        time.sleep(10)\n",
    "        # Nothing new here\n",
    "        hrefs = response.xpath(\"//div[@id='bodyContent']//a[@href[re:test(.,'^(/wiki/)((?!:).)*$')]]/@href\").getall()\n",
    "        full_text = \"\"\n",
    "        for para in response.xpath(\"//p/text()\").getall():\n",
    "            full_text += para + \" \"\n",
    "        \n",
    "        # Scrapy works based on scrapy request, the most important argument being callback\n",
    "        # Basically you get an url and call a function to work on this url (in this case the same as for starting_url: parse())\n",
    "        for url in hrefs:\n",
    "            yield(scrapy.Request(url=\"https://en.wikipedia.org\" + url, callback=self.parse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd44a860",
   "metadata": {},
   "source": [
    "To run it just go the spiders folder and run in a console:\n",
    "\n",
    "```console\n",
    "scrapy runspider spider_wikipedia.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681c39be",
   "metadata": {},
   "source": [
    "At this point if you run the spider it won't give you any results and your terminal should look like this:\n",
    "\n",
    "![robots](img/robots.png)\n",
    "\n",
    "Seems like there's some kind of issue with the following url: https://fr.wikipedia.org/robots.txt. \n",
    "Turns out website don't like that robots try to scrap them, basic behavior of scrapy is to respect this rules. Indeed if you look at scrapy_ap/settings.py you'll find the following:\n",
    "\n",
    "```\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = True\n",
    "```\n",
    "\n",
    "robots.txt are crucial information but for the sake of the tutorial, and since our goal is not to overflow wikipedia's server, we will turn down this setting to False:\n",
    "\n",
    "```\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    "```\n",
    "\n",
    "If you try to run this now it should work although we did not put an ending condition so it will run forever so don't do it ! \n",
    "Before going into more details on the spider let's focus on settings now that we introduced a bit scrapy_ap/settings.py. Indeed there's a lot of commented line and a lot of features you can enable/disable to avoid complex coding scheme. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc866426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapy settings for scrapy_wiki project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "BOT_NAME = 'scrapy_wiki'\n",
    "\n",
    "SPIDER_MODULES = ['scrapy_wiki.spiders']\n",
    "NEWSPIDER_MODULE = 'scrapy_wiki.spiders'\n",
    "\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "#USER_AGENT = 'scrapy_wiki (+http://www.yourdomain.com)'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
    "#CONCURRENT_REQUESTS = 32\n",
    "\n",
    "# Configure a delay for requests for the same website (default: 0)\n",
    "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
    "# See also autothrottle settings and docs\n",
    "#DOWNLOAD_DELAY = 3\n",
    "# The download delay setting will honor only one of:\n",
    "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
    "#CONCURRENT_REQUESTS_PER_IP = 16\n",
    "\n",
    "# Disable cookies (enabled by default)\n",
    "#COOKIES_ENABLED = False\n",
    "\n",
    "# Disable Telnet Console (enabled by default)\n",
    "#TELNETCONSOLE_ENABLED = False\n",
    "\n",
    "# Override the default request headers:\n",
    "#DEFAULT_REQUEST_HEADERS = {\n",
    "#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "#   'Accept-Language': 'en',\n",
    "#}\n",
    "\n",
    "# Enable or disable spider middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "#SPIDER_MIDDLEWARES = {\n",
    "#    'scrapy_wiki.middlewares.ScrapyWikiSpiderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable downloader middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#DOWNLOADER_MIDDLEWARES = {\n",
    "#    'scrapy_wiki.middlewares.ScrapyWikiDownloaderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable extensions\n",
    "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
    "#EXTENSIONS = {\n",
    "#    'scrapy.extensions.telnet.TelnetConsole': None,\n",
    "#}\n",
    "\n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "#ITEM_PIPELINES = {\n",
    "#    'scrapy_wiki.pipelines.ScrapyWikiPipeline': 300,\n",
    "#}\n",
    "\n",
    "# Enable and configure the AutoThrottle extension (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
    "#AUTOTHROTTLE_ENABLED = True\n",
    "# The initial download delay\n",
    "#AUTOTHROTTLE_START_DELAY = 5\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "#AUTOTHROTTLE_MAX_DELAY = 60\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "#AUTOTHROTTLE_DEBUG = False\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
    "#HTTPCACHE_ENABLED = True\n",
    "#HTTPCACHE_EXPIRATION_SECS = 0\n",
    "#HTTPCACHE_DIR = 'httpcache'\n",
    "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
    "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ecd6b",
   "metadata": {},
   "source": [
    "But settings are not the only place you can find nice features, you already have seen that allowed_domains avoid getting caught up in weird website, another example is the duplicate ignore feature. Adding dont_filter=True to scrapy.Request() will ignore some of these features. Look there https://doc.scrapy.org/en/latest/topics/request-response.html#request-objects for more scrapy.Request() arguments, pretty sure you'll find some things that are useful for you.\n",
    "\n",
    "Now we talked about settings.py and spiders but there's still a lot of files left, why are they here ? Well as said above it's meant to have a more structured code and not a single file with every operation you do.\n",
    "\n",
    "- items.py is made to handle and restrict the data retrieved from your request.\n",
    "- pipelines.py will process items (clean, saving in mongo, ...).\n",
    "- middlewares.py will process request and response.\n",
    "\n",
    "Let's start with item.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b61e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# item.py\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class ScrapyWikiItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9918c1",
   "metadata": {},
   "source": [
    "We want to create an item that stores the text, an id and the href."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b58df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# item.py\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class ScrapyWikiItem(scrapy.Item):\n",
    "    # Read the documentation, scrapy.Field() basic item object\n",
    "    id_ = scrapy.Field()\n",
    "    href = scrapy.Field()\n",
    "    text = scrapy.Field()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd0fda",
   "metadata": {},
   "source": [
    "We can then modify spider spider_wikipedia.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643f3535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider_wikipedia.py \n",
    "\n",
    "import scrapy\n",
    "from scrapy_wiki.items import ScrapyWikiItem\n",
    "\n",
    "\n",
    "# A class that inherits from scrapy.Spider. We will see in Chap 2 what inheritance is for the moment just know that we \"inherit\" modules from the class scrapy.Spider\n",
    "# This means that you have some function and features already implemented and usable.\n",
    " \n",
    "class SpiderWikipediaSpider(scrapy.Spider):\n",
    "    # the name we introduce during the creation of the spider\n",
    "    name = 'spider_wikipedia'\n",
    "    # If you try to scrap an url outside of allowed_domains it wont work\n",
    "    allowed_domains = ['wikipedia.org']\n",
    "    # The first url you will parse\n",
    "    start_urls = [\"https://en.wikipedia.org/wiki/Kevin_Bacon\"]\n",
    "    # create a counter\n",
    "    n_processed = 0\n",
    "\n",
    "    # What you do with the first url, response = what we get with a request.get()\n",
    "    def parse(self, response):\n",
    "\n",
    "        hrefs = response.xpath(\"//div[@id='bodyContent']//a[@href[re:test(.,'^(/wiki/)((?!:).)*$')]]/@href\").getall()\n",
    "        # update counter\n",
    "        self.n_processed += 1\n",
    "        # create instance of item\n",
    "        item = ScrapyWikiItem()\n",
    "        item[\"href\"] = response.url\n",
    "        item[\"text\"] = response.xpath(\"//p/text()\").getall()\n",
    "        item[\"id_\"] = self.n_processed\n",
    "        \n",
    "        # Scrapy works based on scrapy request, the most important argument being callback\n",
    "        # Basically you get an url and call a function to work on this url (in this case the same as for starting_url: parse())\n",
    "        for url in hrefs:\n",
    "            # meta if you want to update item as you go along, in this case not needed\n",
    "            yield scrapy.Request(url=\"https://en.wikipedia.org\" + url, callback=self.parse,meta={'item': item})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e7062d",
   "metadata": {},
   "source": [
    "Notice how we do not process the text for the moment, we will use pipelines.py to do it. For the moment the item is just returned (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af210ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines.py\n",
    "\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "\n",
    "class ScrapyWikiPipeline(object):\n",
    "    def process_item(self, item, spider):\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4716d8",
   "metadata": {},
   "source": [
    "Now we want to clean the text and put it in a mongodb, we start from a code given in the documentation (https://docs.scrapy.org/en/latest/topics/item-pipeline.html) and just add a clean_text function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459c94a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines.py\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "\n",
    "# useful for handling different item types with a single interface\n",
    "import re\n",
    "import pymongo\n",
    "from itemadapter import ItemAdapter\n",
    "\n",
    "\n",
    "class MongoPipeline:\n",
    "\n",
    "    collection_name = 'scrapy'\n",
    "\n",
    "    def __init__(self, mongo_uri, mongo_db):\n",
    "        self.mongo_uri = mongo_uri\n",
    "        self.mongo_db = mongo_db\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        return cls(\n",
    "            mongo_uri=crawler.settings.get('MONGO_URI'),\n",
    "            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')\n",
    "        )\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.client = pymongo.MongoClient(self.mongo_uri)\n",
    "        self.db = self.client[self.mongo_db]\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.client.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        item[\"text\"] = self.clean_text(item[\"text\"])\n",
    "        self.db[self.collection_name].insert_one(ItemAdapter(item).asdict())\n",
    "        return item\n",
    "    \n",
    "    \n",
    "    def clean_text(self,text):\n",
    "        full_text = re.sub(\"\\n\",\"\",\" \".join(text))\n",
    "        return full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9d7b08",
   "metadata": {},
   "source": [
    " You also need to enable pipelines in the settings and give the DB name and URI in settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6572292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings.py\n",
    "\n",
    "ITEM_PIPELINES = {\n",
    "    'scrapy_wiki.pipelines.MongoPipeline': 300,\n",
    "}\n",
    "\n",
    "MONGO_URI = \"mongodb://localhost:27017\"\n",
    "MONGO_DATABASE = \"M2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe8c9ca",
   "metadata": {},
   "source": [
    "Finally you need to add a yield to the spider so that it knows to process the item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f319c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider_wikipedia.py \n",
    "\n",
    "import scrapy\n",
    "from scrapy_wiki.items import ScrapyWikiItem\n",
    "\n",
    "\n",
    "# A class that inherits from scrapy.Spider. We will see in Chap 2 what inheritance is for the moment just know that we \"inherit\" modules from the class scrapy.Spider\n",
    "# This means that you have some function and features already implemented and usable.\n",
    " \n",
    "class SpiderWikipediaSpider(scrapy.Spider):\n",
    "    # the name we introduce during the creation of the spider\n",
    "    name = 'spider_wikipedia'\n",
    "    # If you try to scrap an url outside of allowed_domains it wont work\n",
    "    allowed_domains = ['wikipedia.org']\n",
    "    # The first url you will parse\n",
    "    start_urls = [\"https://en.wikipedia.org/wiki/Kevin_Bacon\"]\n",
    "    # create a counter\n",
    "    n_processed = 0\n",
    "\n",
    "    # What you do with the first url, response = what we get with a request.get()\n",
    "    def parse(self, response):\n",
    "\n",
    "        hrefs = response.xpath(\"//div[@id='bodyContent']//a[@href[re:test(.,'^(/wiki/)((?!:).)*$')]]/@href\").getall()\n",
    "        # update counter\n",
    "        self.n_processed += 1\n",
    "        # create instance of item\n",
    "        item = ScrapyWikiItem()\n",
    "        item[\"href\"] = response.url\n",
    "        item[\"text\"] = response.xpath(\"//p/text()\").getall()\n",
    "        item[\"id_\"] = self.n_processed\n",
    "        \n",
    "        yield item\n",
    "        # Scrapy works based on scrapy request, the most important argument being callback\n",
    "        # Basically you get an url and call a function to work on this url (in this case the same as for starting_url: parse())\n",
    "        for url in hrefs:\n",
    "            # meta if you want to update item as you go along, in this case not needed\n",
    "            yield scrapy.Request(url=\"https://en.wikipedia.org\" + url, callback=self.parse,meta={'item': item})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc82a6",
   "metadata": {},
   "source": [
    "And that's it ! You have your first scrapy project ! There's of course much more to see and we still haven't talked about middlewares.py but we stop here for the moment.\n",
    "Ok so now one would think you have all the tools to scrap websites, well think again ! Let's try to see using scrapy shell what you get when scraping twitch for example:\n",
    "\n",
    "\n",
    "```console\n",
    "scrapy shell\n",
    "fetch(\"https://www.twitch.tv/\")\n",
    "view(response)\n",
    "```\n",
    "\n",
    "![twitch](img/twitch.png)\n",
    "\n",
    "\n",
    "Seems like it does not load. This is due to JavaScript. At some point in time the www was only html and css and the scrapping was easier. Today almost every website you use have some javascript runnning in the background making it dynamic. This makes it hard for BS and Scrapy to find what they are looking for. Now comes a new library called Selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a77df1",
   "metadata": {},
   "source": [
    "<a name=\"Selenium\"></a>\n",
    "## Selenium\n",
    "\n",
    "Selenium is meant to act as if a human was using a web browser. This means that you need a web browser for it to work (we will use mozilla but chrome or others are fine too) and a driver (specific for the browser, geckodriver is for mozilla). DL geckodriver here https://github.com/mozilla/geckodriver/releases. Let's start again with twitch. When you start a code with Selenium you should have a page that opens up (default behavior that can be changed), this page is called \"marionette\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f9a7a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Start up the marionette\n",
    "driver = webdriver.Firefox()\n",
    "# go to this page\n",
    "driver.get(\"https://www.twitch.tv/directory\")\n",
    "\n",
    "# Get infos\n",
    "publications_href = driver.find_elements(By.XPATH, \"//a[@class='ScCoreLink-sc-udwpw5-0 lpnppF tw-link']\")\n",
    "urls = [ref.get_attribute('href') for ref in publications_href]\n",
    "print(urls)\n",
    "# Close marionette\n",
    "#driver.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69af0acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.twitch.tv/directory/game/Just%20Chatting', 'https://www.twitch.tv/directory/game/Just%20Chatting', 'https://www.twitch.tv/directory/game/Fortnite', 'https://www.twitch.tv/directory/game/Fortnite', 'https://www.twitch.tv/directory/game/League%20of%20Legends', 'https://www.twitch.tv/directory/game/League%20of%20Legends', 'https://www.twitch.tv/directory/game/Rocket%20League', 'https://www.twitch.tv/directory/game/Rocket%20League', 'https://www.twitch.tv/directory/game/Grand%20Theft%20Auto%20V', 'https://www.twitch.tv/directory/game/Grand%20Theft%20Auto%20V', 'https://www.twitch.tv/directory/game/Minecraft', 'https://www.twitch.tv/directory/game/Minecraft', 'https://www.twitch.tv/directory/game/Call%20of%20Duty%3A%20Warzone', 'https://www.twitch.tv/directory/game/Call%20of%20Duty%3A%20Warzone', 'https://www.twitch.tv/directory/game/VALORANT', 'https://www.twitch.tv/directory/game/VALORANT', 'https://www.twitch.tv/directory/game/Apex%20Legends', 'https://www.twitch.tv/directory/game/Apex%20Legends', 'https://www.twitch.tv/directory/game/Road%2096', 'https://www.twitch.tv/directory/game/Road%2096', 'https://www.twitch.tv/directory/game/Twelve%20Minutes', 'https://www.twitch.tv/directory/game/Twelve%20Minutes', 'https://www.twitch.tv/directory/game/Genshin%20Impact', 'https://www.twitch.tv/directory/game/Genshin%20Impact', 'https://www.twitch.tv/directory/game/Fall%20Guys%3A%20Ultimate%20Knockout', 'https://www.twitch.tv/directory/game/Fall%20Guys%3A%20Ultimate%20Knockout', 'https://www.twitch.tv/directory/game/ALTF4', 'https://www.twitch.tv/directory/game/ALTF4', 'https://www.twitch.tv/directory/game/Pok%C3%A9mon%20Emerald', 'https://www.twitch.tv/directory/game/Pok%C3%A9mon%20Emerald', 'https://www.twitch.tv/directory/game/Sports', 'https://www.twitch.tv/directory/game/Sports', 'https://www.twitch.tv/directory/game/PICO%20PARK', 'https://www.twitch.tv/directory/game/PICO%20PARK', 'https://www.twitch.tv/directory/game/ASMR', 'https://www.twitch.tv/directory/game/ASMR', 'https://www.twitch.tv/directory/game/Music', 'https://www.twitch.tv/directory/game/Music', 'https://www.twitch.tv/directory/game/Mario%20Kart%208', 'https://www.twitch.tv/directory/game/Mario%20Kart%208', 'https://www.twitch.tv/directory/game/Counter-Strike%3A%20Global%20Offensive', 'https://www.twitch.tv/directory/game/Counter-Strike%3A%20Global%20Offensive', 'https://www.twitch.tv/directory/game/Talk%20Shows%20%26%20Podcasts', 'https://www.twitch.tv/directory/game/Talk%20Shows%20%26%20Podcasts', 'https://www.twitch.tv/directory/game/Slots', 'https://www.twitch.tv/directory/game/Slots', 'https://www.twitch.tv/directory/game/Special%20Events', 'https://www.twitch.tv/directory/game/Special%20Events', 'https://www.twitch.tv/directory/game/HUMANKIND', 'https://www.twitch.tv/directory/game/HUMANKIND', 'https://www.twitch.tv/directory/game/The%20Binding%20of%20Isaac%3A%20Repentance', 'https://www.twitch.tv/directory/game/The%20Binding%20of%20Isaac%3A%20Repentance', \"https://www.twitch.tv/directory/game/Garry's%20Mod\", \"https://www.twitch.tv/directory/game/Garry's%20Mod\", 'https://www.twitch.tv/directory/game/Call%20of%20Duty%3A%20Black%20Ops%20Cold%20War', 'https://www.twitch.tv/directory/game/Call%20of%20Duty%3A%20Black%20Ops%20Cold%20War', 'https://www.twitch.tv/directory/game/Pools%2C%20Hot%20Tubs%2C%20and%20Beaches', 'https://www.twitch.tv/directory/game/Pools%2C%20Hot%20Tubs%2C%20and%20Beaches', 'https://www.twitch.tv/directory/game/Diablo%20II%3A%20Resurrected', 'https://www.twitch.tv/directory/game/Diablo%20II%3A%20Resurrected']\n"
     ]
    }
   ],
   "source": [
    "publications_href = driver.find_elements(By.XPATH, \"//a[@class='ScCoreLink-sc-udwpw5-0 lpnppF tw-link']\")\n",
    "urls = [ref.get_attribute('href') for ref in publications_href]\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dc9563",
   "metadata": {},
   "source": [
    "From this short example you can already see two problems from simulating real behavior:\n",
    "\n",
    "- The loading time\n",
    "- Scrolling to load\n",
    "\n",
    "The loading time can be easily avoided adding a wait condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf89afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait # New import\n",
    "\n",
    "# Start up the marionette\n",
    "driver = webdriver.Firefox()\n",
    "\n",
    "# go to this page\n",
    "driver.get(\"https://www.twitch.tv/directory\")\n",
    "# Condition: wait for element, if after 10 second not found then send an error\n",
    "WebDriverWait(driver, 10).until(lambda driver: driver.find_elements(By.XPATH, \"//a[@class='ScCoreLink-sc-udwpw5-0 lpnppF tw-link']\"))\n",
    "\n",
    "# Get infos\n",
    "publications_href = driver.find_elements(By.XPATH, \"//a[@class='ScCoreLink-sc-udwpw5-0 lpnppF tw-link']\")\n",
    "urls = [ref.get_attribute('href') for ref in publications_href]\n",
    "print(urls)\n",
    "# Close marionette\n",
    "#driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7473794",
   "metadata": {},
   "source": [
    "Scrolling takes a bit more coding to deal with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6777d1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% scrolling function example\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import time\n",
    "\n",
    "def scrolldown(driver,bottom = False, n = 0 ):\n",
    "    SCROLL_PAUSE_TIME = 2\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if bottom == True:\n",
    "        while True:\n",
    "            # Scroll down to bottom\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            # Wait to load page\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "    else:\n",
    "        for i in range(n):            \n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            last_height = new_height\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(\"https://twitter.com/anacondainc\")\n",
    "scrolldown(driver,bottom=False,n=10)\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ec6cbc",
   "metadata": {},
   "source": [
    "Although this function is pretty general it does not work in some specific case and you need to adapt, improvise and overcome and ActionChains might come in handy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cccd2a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% this scrolling does not work in all case:\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait    \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(\"https://www.twitch.tv/directory\")\n",
    "WebDriverWait(driver, 10).until(lambda driver: driver.find_elements(By.XPATH, \"//a[@class='ScCoreLink-sc-udwpw5-0 lpnppF tw-link']\"))\n",
    "\n",
    "def scroll_twitch(driver,n=0):\n",
    "    element = driver.find_element_by_xpath(\"//div[@class='Layout-sc-nxg1ff-0 imInLb']//h1[@class='CoreText-sc-cpl358-0 ScTitleText-sc-1gsen4-0 ipNmNI tw-title']\")\n",
    "    element.click()\n",
    "    for i in range(n):\n",
    "        try:\n",
    "            action_chains = ActionChains(driver)\n",
    "            action_chains.send_keys(Keys.PAGE_DOWN).perform()\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            \n",
    "\n",
    "scroll_twitch(driver,10)\n",
    "\n",
    "\n",
    "\n",
    "# every action chains http://www.allselenium.info/python-selenium-all-mouse-actions-using-actionchains/#clickandhold(onelement=None)\n",
    "# Problem : the bar is longer so you can scroll down the same amount: Hands-on, decay over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb5ce8",
   "metadata": {},
   "source": [
    "Last thing to see is how to login using Selenium. Some website require authentification to perform certain action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9952d703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndriver.add_cookie({'name': 'twitch.lohp.countryCode',\\n  'value': 'GE',\\n  'path': '/',\\n  'domain': '.twitch.tv',\\n  'secure': False,\\n  'httpOnly': False,\\n  'expiry': 1912091189})\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import pickle\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(\"https://www.twitch.tv/login\")\n",
    "\n",
    "user_name = \"username\"\n",
    "password = \"password\"\n",
    "\n",
    "element = driver.find_element_by_id(\"login-username\")\n",
    "element.send_keys(user_name)\n",
    "\n",
    "element = driver.find_element_by_id(\"password-input\")\n",
    "element.send_keys(password)\n",
    "\n",
    "sign_in = driver.find_element(By.XPATH, \"//button[@data-a-target='passport-login-button']\")\n",
    "sign_in.click()\n",
    "\n",
    "# cookies\n",
    "\n",
    "\n",
    "driver.get_cookies()\n",
    "pickle.dump( driver.get_cookies() , open(\"data/cookies_twitch.pkl\",\"wb\"))\n",
    "\n",
    "cookies = pickle.load(open(\"data/cookies_twitch.pkl\", \"rb\"))\n",
    "for cookie in cookies:\n",
    "    driver.add_cookie(cookie)\n",
    "    \n",
    "\"\"\"\n",
    "driver.add_cookie({'name': 'twitch.lohp.countryCode',\n",
    "  'value': 'GE',\n",
    "  'path': '/',\n",
    "  'domain': '.twitch.tv',\n",
    "  'secure': False,\n",
    "  'httpOnly': False,\n",
    "  'expiry': 1912091189})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d58c32",
   "metadata": {},
   "source": [
    "Now you might have noticed that the structure is similar to BS (+ Xpath).\n",
    "A nice thing could be to have the scrapy structure with a Selenium backend. To do this we will use the middlewares.py of scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f5f931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#middlewares.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define here the models for your spider middleware\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "from scrapy import signals\n",
    "\n",
    "from scrapy.http import HtmlResponse\n",
    "\n",
    "class ScrapyWikiSpiderMiddleware:\n",
    "    # Not all methods need to be defined. If a method is not defined,\n",
    "    # scrapy acts as if the spider middleware does not modify the\n",
    "    # passed objects.\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        # This method is used by Scrapy to create your spiders.\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_spider_input(self, response, spider):\n",
    "        # Called for each response that goes through the spider\n",
    "        # middleware and into the spider.\n",
    "\n",
    "        # Should return None or raise an exception.\n",
    "        return None\n",
    "\n",
    "    def process_spider_output(self, response, result, spider):\n",
    "        # Called with the results returned from the Spider, after\n",
    "        # it has processed the response.\n",
    "\n",
    "        # Must return an iterable of Request, dict or Item objects.\n",
    "        for i in result:\n",
    "            yield i\n",
    "\n",
    "    def process_spider_exception(self, response, exception, spider):\n",
    "        # Called when a spider or process_spider_input() method\n",
    "        # (from other spider middleware) raises an exception.\n",
    "\n",
    "        # Should return either None or an iterable of Request, dict\n",
    "        # or Item objects.\n",
    "        pass\n",
    "\n",
    "    def process_start_requests(self, start_requests, spider):\n",
    "        # Called with the start requests of the spider, and works\n",
    "        # similarly to the process_spider_output() method, except\n",
    "        # that it doesn’t have a response associated.\n",
    "\n",
    "        # Must return only requests (not items).\n",
    "        for r in start_requests:\n",
    "            yield r\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info('Spider opened: %s' % spider.name)\n",
    "\n",
    "\n",
    "class ScrapyWikiDownloaderMiddleware(object):\n",
    "    # Not all methods need to be defined. If a method is not defined,\n",
    "    # scrapy acts as if the downloader middleware does not modify the\n",
    "    # passed objects.\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        # This method is used by Scrapy to create your spiders.\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_request(self, request, spider):\n",
    "        return None\n",
    "\n",
    "    def process_response(self, request, response, spider):\n",
    "        # Called with the response returned from the downloader.\n",
    "\n",
    "        # Must either;\n",
    "        # - return a Response object\n",
    "        # - return a Request object\n",
    "        # - or raise IgnoreRequest\n",
    "        return response\n",
    "\n",
    "    def process_exception(self, request, exception, spider):\n",
    "        # Called when a download handler or a process_request()\n",
    "        # (from other downloader middleware) raises an exception.\n",
    "\n",
    "        # Must either:\n",
    "        # - return None: continue processing this exception\n",
    "        # - return a Response object: stops process_exception() chain\n",
    "        # - return a Request object: stops process_exception() chain\n",
    "        pass\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info('Spider opened: %s' % spider.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8591e40c",
   "metadata": {},
   "source": [
    "The thing we want to modify is the process_request() function. Instead of getting the simple response of the request.get(), we will use Selenium to send back a driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d087ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#middlewares.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define here the models for your spider middleware\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "from scrapy import signals\n",
    "from scrapy.http import HtmlResponse\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "\n",
    "class ScrapyWikiSpiderMiddleware:\n",
    "    # Not all methods need to be defined. If a method is not defined,\n",
    "    # scrapy acts as if the spider middleware does not modify the\n",
    "    # passed objects.\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        # This method is used by Scrapy to create your spiders.\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_spider_input(self, response, spider):\n",
    "        # Called for each response that goes through the spider\n",
    "        # middleware and into the spider.\n",
    "\n",
    "        # Should return None or raise an exception.\n",
    "        return None\n",
    "\n",
    "    def process_spider_output(self, response, result, spider):\n",
    "        # Called with the results returned from the Spider, after\n",
    "        # it has processed the response.\n",
    "\n",
    "        # Must return an iterable of Request, dict or Item objects.\n",
    "        for i in result:\n",
    "            yield i\n",
    "\n",
    "    def process_spider_exception(self, response, exception, spider):\n",
    "        # Called when a spider or process_spider_input() method\n",
    "        # (from other spider middleware) raises an exception.\n",
    "\n",
    "        # Should return either None or an iterable of Request, dict\n",
    "        # or Item objects.\n",
    "        pass\n",
    "\n",
    "    def process_start_requests(self, start_requests, spider):\n",
    "        # Called with the start requests of the spider, and works\n",
    "        # similarly to the process_spider_output() method, except\n",
    "        # that it doesn’t have a response associated.\n",
    "\n",
    "        # Must return only requests (not items).\n",
    "        for r in start_requests:\n",
    "            yield r\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info('Spider opened: %s' % spider.name)\n",
    "\n",
    "\n",
    "class ScrapyWikiDownloaderMiddleware(object):\n",
    "    # Not all methods need to be defined. If a method is not defined,\n",
    "    # scrapy acts as if the downloader middleware does not modify the\n",
    "    # passed objects.\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        # This method is used by Scrapy to create your spiders.\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_request(self, request, spider):\n",
    "        driver.get(request.url)\n",
    "        body = driver.page_source\n",
    "        return HtmlResponse(driver.current_url, body=body, encoding='utf-8', request=request)\n",
    "    \n",
    "    def process_response(self, request, response, spider):\n",
    "        # Called with the response returned from the downloader.\n",
    "\n",
    "        # Must either;\n",
    "        # - return a Response object\n",
    "        # - return a Request object\n",
    "        # - or raise IgnoreRequest\n",
    "        return response\n",
    "\n",
    "    def process_exception(self, request, exception, spider):\n",
    "        # Called when a download handler or a process_request()\n",
    "        # (from other downloader middleware) raises an exception.\n",
    "\n",
    "        # Must either:\n",
    "        # - return None: continue processing this exception\n",
    "        # - return a Response object: stops process_exception() chain\n",
    "        # - return a Request object: stops process_exception() chain\n",
    "        pass\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info('Spider opened: %s' % spider.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c0c673",
   "metadata": {},
   "source": [
    "Last thing to do enable the middlewares in settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa2f28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOADER_MIDDLEWARES = {\n",
    "    'ScrapyWiki.middlewares.ScrapyApDownloaderMiddleware': 543,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cfde5b",
   "metadata": {},
   "source": [
    "Of course this method is not failproof. Sometimes you'll want to return the driver and not just the response for example. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67303d5",
   "metadata": {},
   "source": [
    "<a name=\"API\"></a>\n",
    "## APIs\n",
    "\n",
    "We have seen how to scrap information from website directly. Althoug this seems like a safe methods it is not the best for the server. Most of the time you don't require the whole page but some specific information on this page. Developpers that are ok with you scraping their website have probably implemented some kind of Application Programming Interface (API). This API reduces the overhead of your query and gives you only the data your are interested in. Querying an api is usually as easy as request.get() if documentation are available. Let's see some small example:\n",
    "\n",
    "(sidenote: API means nothing and everything at the same time. When you are using your phone to send a message you use an API, when you use a library in python you use an API, when you open a webbrowser you use an API... Be wary when you use this acronym !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7b8d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Arxiv\n",
    "\n",
    "# First objective find if there's an API and the documentation of the API\n",
    "# https://arxiv.org/help/api/tou\n",
    "\n",
    "import requests\n",
    "import feedparser\n",
    "\n",
    "\n",
    "response = requests.get('http://export.arxiv.org/api/query?search_query=all:electron&start=0&max_results=10')\n",
    "feed = feedparser.parse(response.content)\n",
    "feed\n",
    "\n",
    "results = {}\n",
    "for entry in feed.entries:\n",
    "    print(entry)\n",
    "    results[entry.id] = {\"title\": entry.title,\n",
    "                         \"abstract\":entry.summary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2099fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Twitch\n",
    "\n",
    "# https://dev.twitch.tv/docs/authentication\n",
    "# https://dev.twitch.tv/docs/api\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pymongo\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "client = pymongo.MongoClient('localhost',27017)\n",
    "\n",
    "mydb = client[\"api_twitch\"]\n",
    "collection = mydb[\"top_games\"]\n",
    "\n",
    "\n",
    "Client_ID = \"zuxz59ow9v8zncx3ljdyo5jaj1sqdz\"\n",
    "secret = \"julw385uytn4dhzkublz2wa644l3he\"\n",
    "\n",
    "access_token = requests.post(\"https://id.twitch.tv/oauth2/token?client_id={}&client_secret={}&grant_type=client_credentials\".format(Client_ID,secret))\n",
    "access_token = json.loads(access_token.content)[\"access_token\"]\n",
    "#scope = \"analytics:read:games\"\n",
    "headers = {\"Client-ID\": Client_ID, \"Authorization\": \"Bearer \" + access_token,}\n",
    "\n",
    "n_games = 40\n",
    "limit = 20\n",
    "n_iteration = int(n_games/limit)\n",
    "\n",
    "for i in tqdm.tqdm(range(n_iteration)):\n",
    "    if i == 0:\n",
    "        response_category = requests.get(\"https://api.twitch.tv/helix/games/top\",headers = headers)\n",
    "    else:\n",
    "        response_category = requests.get(\"https://api.twitch.tv/helix/games/top?after={}\".format(json.loads(response_category.content)[\"pagination\"][\"cursor\"]),headers = headers)\n",
    "    for category in json.loads(response_category.content)[\"data\"]:\n",
    "        response = requests.get('https://api.twitch.tv/helix/streams?game_id={}'.format(category[\"id\"]), headers=headers)\n",
    "        streamers = {}\n",
    "        for streamer in json.loads(response.content)[\"data\"]:\n",
    "            streamers[streamer[\"user_id\"]] = {\"user_name\":streamer[\"user_name\"],\n",
    "                                              \"title\":streamer[\"title\"],\n",
    "                                              \"viewer_count\":streamer[\"viewer_count\"],\n",
    "                                              \"started_at\":streamer[\"started_at\"],\n",
    "                                              \"language\":streamer[\"language\"],} \n",
    "        \n",
    "        done = False\n",
    "        while done == False:\n",
    "            try:\n",
    "                response = requests.get('https://api.twitch.tv/helix/streams?game_id={}&after={}'.format(category[\"id\"],json.loads(response.content)[\"pagination\"][\"cursor\"]), headers=headers)\n",
    "                for streamer in json.loads(response.content)[\"data\"]:\n",
    "                    streamers[streamer[\"user_id\"]] = {\"user_name\":streamer[\"user_name\"],\n",
    "                                                      \"title\":streamer[\"title\"],\n",
    "                                                      \"viewer_count\":streamer[\"viewer_count\"],\n",
    "                                                      \"started_at\":streamer[\"started_at\"],\n",
    "                                                      \"language\":streamer[\"language\"],}\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                done = True\n",
    "        \n",
    "        post = {\"_id\": category[\"id\"],\n",
    "                \"game\": category[\"name\"],\n",
    "                \"streamers\": streamers,\n",
    "                }\n",
    "        try:\n",
    "            collection.insert_one(post)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            \n",
    "    print(response.headers)\n",
    "    time.sleep(1)\n",
    "    \n",
    "\n",
    "\n",
    "cursor = collection.find({\"game\":\"World of Warcraft\"})\n",
    "for document in cursor:\n",
    "    print(document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd680f9",
   "metadata": {},
   "source": [
    "Depending on the api you'll have to work with different data format. The most popular is json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14888cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "response = requests.get(\"http://ip-api.com/json/50.78.253.58\")\n",
    "response.content\n",
    "json_str = json.loads(response.content)\n",
    "json_str[\"city\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399d72ee",
   "metadata": {},
   "source": [
    "The other less popular format is xml. The next code will show you how to parse it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62478508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('source', 'Radiat. Meas.'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'J. Asian Earth Sci.'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Archaeometry'),\n",
       " ('source', 'Archaeometry'),\n",
       " ('source', 'Quat. Int.'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Quat. Int.'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Quat. Int.'),\n",
       " ('source', 'Geochronometria'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Nature'),\n",
       " ('source', 'Boreas'),\n",
       " ('source', 'Boreas'),\n",
       " ('source', 'Archaeometry'),\n",
       " ('source', 'Thermoluminescence Dating'),\n",
       " ('source', 'Geochronometria'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Quat. Int.'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Boreas'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Quat. Sci. Rev.'),\n",
       " ('source', 'Quat. Sci. Rev.'),\n",
       " ('source',\n",
       "  'Geologia Cuaternarului: Notiuni de Baza; (Quaternary Geology. Basic Notions)'),\n",
       " ('source', 'Bulletin de l’Association Française Pour L’étude du Quaternaire'),\n",
       " ('source',\n",
       "  'Formatiuni Cuaternareîn Dobrogea (Loessuri si Paleosoluri) (Quaternary Units in Dobrogea)'),\n",
       " ('source', 'Phys. Chem. Earth A'),\n",
       " ('source', 'Rom. Rep. Phys.'),\n",
       " ('source', 'Quaternaire'),\n",
       " ('source', 'Quat. Int.'),\n",
       " ('source', 'Anu. Inst. Geol. României'),\n",
       " ('source', 'Quat. Int.'),\n",
       " ('source', 'Quat. Sci. Rev.'),\n",
       " ('source', 'Quat. Int.'),\n",
       " ('source', 'Catena'),\n",
       " ('source', 'Rom. Rep. Phys.'),\n",
       " ('source', 'Quat. Int'),\n",
       " ('source', 'Earth Planet. Sci. Lett.'),\n",
       " ('source', 'Geology'),\n",
       " ('source', 'Paleoceanography'),\n",
       " ('source', 'Paleoceanography'),\n",
       " ('source', 'Quat. Int.'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Quat. Int.'),\n",
       " ('source', 'Quat. Int.'),\n",
       " ('source', 'Quat. Sci. Rev.'),\n",
       " ('source', 'Anc. TL'),\n",
       " ('source', 'Anc. TL'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Anc. TL'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Radiat. Meas.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% XML\n",
    "# XML = a common language when doing requests. Extensible Markup Language. tree-like structure.\n",
    "# Multiple package to work with python and xml: lxml, xml.dom.minidom, xml.etree.ElementTree\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom\n",
    "\n",
    "xml_file = \"data/Chap1/mps-03-00019.nxml\"\n",
    "\n",
    "# ET\n",
    "tree = ET.parse(xml_file)\n",
    "root = tree.getroot()\n",
    "\n",
    "[(elem.tag, elem.text) for elem in root.iter()]\n",
    "[(elem.tag, elem.text) for elem in root.iter() if elem.tag ==\"source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f0f2eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The loess-paleosol archive from Mircea Vodă (Romania) represents one of the most studied sections in Europe. We are applying here the current state of the art luminescence dating protocols for revisiting the chronology of this section. Analysis were performed on fine (4–11 µm) and coarse (63–90 µm) quartz extracts using the single aliquot regenerative (SAR) optically stimulated luminescence (OSL) dating protocol. Laboratory generated SAR dose response curves in the high dose range (5 kGy for fine quartz and 2 kGy for coarse quartz) were investigated by employing a test dose of either 17 or 170 Gy. The results confirm the previously reported different saturation characteristics of the two quartz fractions, with no evident dependency of the equivalent dose (D'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# minidom\n",
    "doc = xml.dom.minidom.parse(xml_file) \n",
    "\n",
    "abstract = doc.getElementsByTagName(\"abstract\")\n",
    "body = doc.getElementsByTagName(\"body\")\n",
    "title = doc.getElementsByTagName(\"title-group\")\n",
    "figures = doc.getElementsByTagName(\"fig\")\n",
    "\n",
    "abstract[0].childNodes[0].childNodes[0].nodeValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c23cda11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timar-Gabor\n"
     ]
    }
   ],
   "source": [
    "#lxml\n",
    "\n",
    "from lxml import etree\n",
    "\n",
    "root = etree.parse(xml_file)\n",
    "abstract = root.xpath(\"//abstract//text()\")\n",
    "body = root.xpath(\"//body//text()\")\n",
    "title = root.xpath(\"//title-group//text()\")\n",
    "figures = root.xpath(\"//fig//text()\")\n",
    "\n",
    "aff = root.xpath(\"//aff/text()\")\n",
    "aff = [i for i in aff if not i.startswith((' ', '\\t'))]\n",
    "aff_label = root.xpath(\"//aff/label/text()\")\n",
    "\n",
    "mails =root.xpath(\"//author-notes/corresp\")[0]\n",
    "mails.getchildren()\n",
    "\n",
    "xref = {}\n",
    "for affiliation,label in zip(aff,aff_label):\n",
    "    xref[label]= affiliation\n",
    "\n",
    "authors = root.xpath(\"//contrib\")\n",
    "authors = [i.getchildren() for i in authors]\n",
    "for author in authors:\n",
    "    names = [i.getchildren() for i in author if i.tag == \"name\"][0]\n",
    "    surname = [i.text for i in names if i.tag==\"surname\"]\n",
    "    name = [i.text for i in names if i.tag==\"given-names\"]\n",
    "    xrefs = [i.text for i in author if i.tag==\"xref\"]\n",
    "\n",
    "    \n",
    "print(names[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71d46c3",
   "metadata": {},
   "source": [
    "<a name=\"FTP\"></a>\n",
    "## FTP server\n",
    "\n",
    "Sometimes website don't have APIs but FTP server where you can download bulk of data. Its unlikely that you'll encounter FTP server soon but just in case here is how you do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e36d975d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'221 Goodbye.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ftplib\n",
    "import re\n",
    "import tarfile\n",
    "import os\n",
    "#import shutil\n",
    "\n",
    "# callback for ftp.retrbinary\n",
    "def file_write(data):\n",
    "   local_file.write(data) \n",
    "\n",
    "# connect to ftp_server\n",
    "email = \"email@unistra.fr\"\n",
    "ftp = ftplib.FTP(\"ftp.ncbi.nlm.nih.gov\")\n",
    "ftp.login(user=\"anonymous\",passwd=email)\n",
    "ftp.cwd(\"pub/pmc/oa_package\")\n",
    "\n",
    "# Find tar.gz path\n",
    "\n",
    "tar_gz = False\n",
    "while tar_gz == False:\n",
    "    list_files = ftp.nlst()\n",
    "    if re.search(\"\\.\",list_files[-1]):\n",
    "        tar_gz = True\n",
    "    else:\n",
    "        ftp.cwd(list_files[-1])\n",
    "\n",
    "# In this case we will only take 1 file, dl it and uncompress it\n",
    "\n",
    "link = ftp.pwd() +\"/\" + list_files[0]\n",
    "path = 'data/Chap1/{}'.format(list_files[0])\n",
    "local_file = open(path,\"wb\")\n",
    "ftp.retrbinary(\"RETR \" + link,file_write, blocksize=16384)\n",
    "local_file.close()\n",
    "my_tar = tarfile.open(path)\n",
    "my_tar.extractall(\"data/Chap1/\")\n",
    "my_tar.close()\n",
    "os.unlink(path)\n",
    "\n",
    "ftp.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c000fcf2",
   "metadata": {},
   "source": [
    "<a name=\"rules\"></a>\n",
    "## Rules of good conduct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f849da3c",
   "metadata": {},
   "source": [
    "Scraping is a gray area. Some legal action examples:\n",
    "- eBay vs Bidder’s Edge (Too much scraping)\n",
    "- US vs Auernheimer (Taking advantage of a security breach)\n",
    "- Field vs Google (Google scraping and caching a website)\n",
    "\n",
    "Best way to avoid problems:\n",
    "- Follow the rules (e.g:  robots.txt, ToS).\n",
    "- Don’t overload the server (Avoid parallelism and put sometime.sleep()).\n",
    "- Scrap when there’s less users (night).\n",
    "- Contact the authors of the website to let them know what you aredoing.\n",
    "- Use an API if there’s one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238e4d44",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "Code review:\n",
    "- https://github.com/matthpn2/Web-Scraping-with-Beautiful-Soup\n",
    "- https://github.com/SoumitraAgarwal/Fifa-Ratings\n",
    "- https://github.com/RainrainWu/finance_scraper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
